### batch processing or stream processing 

batch processing processes large valume of data at fixed intervals (for example, hourly, monthly , yearly) .
data is collected over time , stored and then processed together as a batch 

suitable for historical data 
const effective 
slightly delay in data availability 

example ; daily sales report generated using glue and spark 

stream processing processes data in real time or near real time as it is generated '
each eventy is processed imideatly or with in second 

used when low latency is requered 
ideal for real time analytics 
more complex and resource intensive 

example ;real time clickstream or fraud detection using kafka / aws kinesis 

one line interview summary :
batch processing handle data in scheduled chunks , while stream processing handles data continuously in real time .

#### what are fact table and dimension table ?
in a data warehouse , data is organized using fact table and dimesion table to support efficient analysis 

fact table 

fact table store quantitative , measurable data related to business process 
contain metrics (number)
ususally very large 
contain forign key that links to dimesion table 

exampe of fact table :
sales amount 
quantity sold 
revenue 
number of transaction

#### dimension table :
a dimesion table store descriptive , contexual information that explain fact 

contains attribute 
smaller in size 
used in filtering , grouping , and labeling data 

example of dimesions:
customer (name, age, location )
product(category , brand )
date(day , month ,year)

fact table store business metrics , while dimension table store discriptive information use to analize those metrics 


#### star schema and snowflake schema 

star schema and snowflake schema are data modeling techniques used in data warehouse to organize fact and dimesion table 

star schema 
a central fact table is directly connected to multiple denortmalized dimension table , forming a star like structure 

key point :
simple design 
faster query performance 
fewer joins 
easy to understand 

best for ; reporting and bi queries 

#### snowflake schema 
in snowflake schema , dimension table normalized into multiple related table , creating a snowflake like structure 

key point : more complex design 
require more joins 
saves storage space 
better for complex dimension 

best for ; complex dimension management 

quick comparison :
structure :
star schema has denormalized dimensions and snowflake shema has normalized dimensions 

query speed: 
star schema 's query speed is faster whie snowflake schema query speed slow 

complexity :
simple for star schema and complex for snowflake schema 

storage :
more for star schema  and less for snowflake schema 

#### apache airflow 
apache airflow is an open source workflow orchestration tool  used to schedule, monitor and manage data pipelines 
its allow data engineer to define workflow as dags (directed acyclic graph ) , where task and their dependencies , are clearly defined
and executed in correct order 

whyapach airflow is used?
workflow orchestration :
manage complex elt/etl pipeline 
task scheduling : runs job at specific intervals 
dependency management : ensure task run in correct order 
monitoring and retries : handle failures, retries, and alerts
scalabilities: support large , complex data workflow 

one line interview summary::
apache airflow is used to orchestrate and schedule data pipeline by managing task dependencies 
execution , monitoring and retries in a scalable way 

### what is dag in apache airflow 
a dag is directed acyclic graph in apache airflow is a logical representation of workflow that define tasks 
and their execution orders

directed : task have a specific direction of execution 
acyclic : no task can depend on itself (no loop)
graph : task are connected beased on dependencies 

each node represent a task, and edge represent dependencies between tasks .

#### why dag is importent ::
control task execution order 
manage dependencies 
enable parallel execution 
improve pipeine reiability 

### what is operator and sensor and hooks in airflow?
in apache airflow , operators , sensors , and hooks are core component used to build and manage workflow 

### operator :
operator define what task need to be executed in dag 

they perform a specific action 
each task in dag is an instance of operator 

example ; PythonOperator:
runs python code 

bashOperator: runs shell command 

EmailOperator:  sends mail 

interview line : operator define actual work to be done in task 

### sensors :
sensors are a spacial type of operator that wait for a condition to be met before moving forward 

they keep cheacking untill a condition is true 
commonly used to wait for data availability 
example :
S3KeySensor:
wait for a file in S3 
ExternalTaskSensor:
wait for another DAG/task 

interview line :
sensor pause execution unitl a specific condition is satisfied 

Hooks :
Hooks provide a connection interface to external system 
handle authntication and connection .
used by operator and sensor 

example :
PostgresHook
S3Hook
Snowflakehook

hooks manage connection to external system and are used internally by operator and sensor 

one line interview summary :
operator do work , sensor wait for condition , and hook manage connection to external system 

#### explain task dependencies in airflow?
task dependencies in airflow define the order in which task are executed with in a dag 
they ensure task run only after its upstream task have completed successfully 

how dependencies are define 
1. using bit shift operator :
task_a>>task_b (task_a run before task_b)
task_c << task_d (task_d run before task_c)

2. using set_upstream() set_downStream()

task_b.set_upstream(task_a)
task_a.downstream(task_b)

#####    how do you monitor airflow pipeline?
airflow pipelien are monitered using a combination of the airflow ui , logs, alert, and external monitoring tools to ensure 

reliability and quick failure detection 

1. airflow ui (primary monitoring tools):
view dag run status (success,failed,running)
track task level states
check execution time and retries 
identify bottlenecks using the graph view and gantt view 

2. logs and debuggin :
each task generate detailed logs 
logs help to identify data issues , timeout error , or connection failure 
logs are accessable directly from airflow ui 

3. alert and notification :
configure email and slack alerts on task/dag failure 
alert can also be sent on sla misses

4. sla monitoring :
uses sla (service level agreement) to check delay in task 
trigger alert when tasks exceed expected execution dtime 

5